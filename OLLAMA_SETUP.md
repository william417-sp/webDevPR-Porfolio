# üöÄ Configuraci√≥n de Ollama Local con Llama 3.2 para el Chatbot

## ‚úÖ El chatbot ya est√° configurado para usar Ollama local con Llama 3.2

El c√≥digo ya est√° listo. Solo necesitas asegurarte de que Ollama est√© corriendo y que tengas el modelo instalado.

## üìù Pasos para Configurar Ollama

### **Paso 1: Verificar que Ollama est√© corriendo**

1. **Inicia Ollama** en tu terminal:
```bash
ollama serve
```

2. **Verifica que est√© corriendo** en: http://localhost:11434
   - Deber√≠as ver algo como: `Ollama is running`

### **Paso 2: Instalar el modelo Llama 3.2**

1. **Instala Llama 3.2**:
```bash
ollama pull llama3.2
```

   **Alternativas:**
   - Llama 3.1: `ollama pull llama3.1`
   - Llama 3: `ollama pull llama3`

2. **Verifica que el modelo est√© instalado**:
```bash
ollama list
```

   Deber√≠as ver `llama3.2` en la lista.

### **Paso 3: Configurar el Chatbot**

1. **Abre el archivo**: `js/ai-chatbot.js`
2. **Busca esta secci√≥n** (al inicio del archivo, l√≠nea ~17):

```javascript
const OLLAMA_CONFIG = {
    enabled: true, // Ya est√° habilitado por defecto
    endpoint: 'http://localhost:11434/api/chat',
    model: 'llama3.2', // ‚¨ÖÔ∏è Aseg√∫rate de que coincida con tu modelo instalado
    ...
};
```

3. **Ajusta el nombre del modelo** si es necesario:
   - Si instalaste `llama3.2` ‚Üí usa `'llama3.2'`
   - Si instalaste `llama3.1` ‚Üí cambia a `'llama3.1'`
   - Si instalaste `llama3` ‚Üí cambia a `'llama3'`

**Ejemplo:**
```javascript
const OLLAMA_CONFIG = {
    enabled: true, // ‚úÖ Habilitado
    endpoint: 'http://localhost:11434/api/chat', // ‚úÖ API local
    model: 'llama3.2', // ‚úÖ Nombre exacto de tu modelo
    temperature: 0.7,
    num_predict: 500
};
```

### **Paso 4: Probar el Chatbot**

1. **Aseg√∫rate de que Ollama est√© corriendo**:
```bash
ollama serve
```

2. **Abre tu p√°gina web** en el navegador
3. **Abre el chatbot** y haz una pregunta de prueba
4. **Verifica en la consola del navegador** (F12) que no haya errores

## üéØ Modelos Disponibles en Ollama

### **Llama 3.2 (Recomendado)**
```bash
ollama pull llama3.2
```
- **Tama√±o**: ~2.2GB (8B) o ~12GB (90B)
- **Mejor calidad** y m√°s reciente

### **Llama 3.1 (Alternativa)**
```bash
ollama pull llama3.1
```
- **Tama√±o**: ~4.7GB (8B)
- Buen balance entre velocidad y calidad

### **Llama 3 (Econ√≥mico)**
```bash
ollama pull llama3
```
- **Tama√±o**: ~4.7GB (8B)
- M√°s r√°pido, menos recursos

## üí° Ventajas de Ollama Local

‚úÖ **100% Privado**: No env√≠a datos a servidores externos
‚úÖ **Sin costos**: Gratis, sin l√≠mites de uso
‚úÖ **R√°pido**: Sin latencia de red
‚úÖ **Sin dependencia de internet**: Funciona offline
‚úÖ **Control total**: Configuraci√≥n completa local

## üõ†Ô∏è Troubleshooting

### **El chatbot no responde con IA**
- ‚úÖ Verifica que `enabled: true`
- ‚úÖ Verifica que Ollama est√© corriendo: `ollama serve`
- ‚úÖ Verifica que tengas el modelo instalado: `ollama list`
- ‚úÖ Abre la consola del navegador (F12) y revisa errores
- ‚úÖ Verifica que el nombre del modelo coincida exactamente

### **Error: Failed to fetch**
- Ollama no est√° corriendo o no es accesible
- Ejecuta: `ollama serve`
- Verifica en el navegador: http://localhost:11434

### **Error: model not found**
- El modelo no est√° instalado
- Instala el modelo: `ollama pull llama3.2`
- Verifica el nombre exacto: `ollama list`

### **Error CORS (Cross-Origin)**
Si usas `localhost` desde un archivo HTML local (`file://`), puede haber problemas de CORS.

**Soluciones:**
1. **Usa un servidor local**:
```bash
# Python
python -m http.server 8000

# Node.js
npx serve

# PHP
php -S localhost:8000
```

2. **O usa el protocolo `http://localhost`** en lugar de `file://`

### **El chatbot usa respuestas predefinidas**
- Esto es normal si Ollama no est√° disponible o hay un error
- El sistema usa fallback autom√°ticamente
- Verifica tu configuraci√≥n y que Ollama est√© corriendo

## üìä Comandos √ötiles de Ollama

```bash
# Iniciar Ollama
ollama serve

# Ver modelos instalados
ollama list

# Instalar un modelo
ollama pull llama3.2

# Ejecutar un modelo directamente (prueba)
ollama run llama3.2

# Ver informaci√≥n de un modelo
ollama show llama3.2

# Eliminar un modelo
ollama rm llama3.2
```

## ‚ú® Caracter√≠sticas

- ‚úÖ **Fallback autom√°tico**: Si Ollama falla, usa respuestas predefinidas
- ‚úÖ **System prompt personalizado**: Configurado para webDevPR
- ‚úÖ **Respuestas en espa√±ol**: Optimizado para tu mercado
- ‚úÖ **Informaci√≥n de la empresa**: El bot conoce tus servicios y precios
- ‚úÖ **100% local y privado**: Sin env√≠o de datos a terceros

## üéØ Pr√≥ximos Pasos

Una vez configurado, el chatbot:
- Responder√° con **Llama 3.2 local** para conversaciones m√°s naturales
- Ser√° **m√°s r√°pido** al estar en local
- Ser√° **100% privado** (sin enviar datos a la nube)
- Mantendr√° las respuestas predefinidas como **fallback** seguro
- Ser√° m√°s inteligente y flexible con preguntas variadas

¬°Listo! Tu chatbot ahora usa Llama 3.2 con Ollama local. üöÄ

## üîí Nota de Seguridad

Si vas a usar esto en producci√≥n en un servidor, considera:
- ‚úÖ Usar HTTPS
- ‚úÖ Implementar rate limiting
- ‚úÖ Validar inputs del usuario
- ‚úÖ Usar un backend proxy para mayor seguridad

